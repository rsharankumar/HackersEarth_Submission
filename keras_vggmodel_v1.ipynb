{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to read image\n",
    "def read_img(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, (256,256))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## set path for images\n",
    "TRAIN_PATH = 'Data/train_img/'\n",
    "TEST_PATH = 'Data/test_img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3215/3215 [00:13<00:00, 233.61it/s]\n",
      "100%|█████████████████████████████████████| 1732/1732 [00:06<00:00, 268.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_img, test_img = [],[]\n",
    "for img_path in tqdm(train['image_id'].values):\n",
    "    train_img.append(read_img(TRAIN_PATH + img_path + '.png'))\n",
    "\n",
    "for img_path in tqdm(test['image_id'].values):\n",
    "    test_img.append(read_img(TEST_PATH + img_path + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize images\n",
    "x_train = np.array(train_img, np.float32) / 255.\n",
    "x_test = np.array(test_img, np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target variable - encoding numeric value\n",
    "label_list = train['label'].tolist()\n",
    "Y_train = {k:v+1 for v,k in enumerate(set(label_list))}\n",
    "y_train = [Y_train[k] for k in label_list]   \n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "#Transfer learning with Inception V3 \n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# Number of product classifications\n",
    "print(y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 26)                8395546   \n",
      "=================================================================\n",
      "Total params: 23,110,234\n",
      "Trainable params: 8,395,546\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## set model architechture \n",
    "add_model = Sequential()\n",
    "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "add_model.add(Dense(256, activation='relu'))\n",
    "add_model.add(Dropout(0.5))\n",
    "add_model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    " \n",
    "#removed to check\n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "\n",
    "\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16   # 32 # tune it\n",
    "epochs = 2 # increase it\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=30, \n",
    "        #rescale=1./255,# newly added\n",
    "        #shear_range=0.2,# newly added\n",
    "        #zoom_range=0.2,# newly added\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1, \n",
    "        horizontal_flip=True)\n",
    "train_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "199/200 [============================>.] - ETA: 185s - loss: 3.2354 - acc: 0.0000e+0 - ETA: 164s - loss: 3.0436 - acc: 0.0938    - ETA: 157s - loss: 3.0696 - acc: 0.083 - ETA: 152s - loss: 3.0296 - acc: 0.093 - ETA: 150s - loss: 3.0218 - acc: 0.087 - ETA: 148s - loss: 3.0439 - acc: 0.072 - ETA: 146s - loss: 3.0392 - acc: 0.080 - ETA: 145s - loss: 3.0262 - acc: 0.085 - ETA: 143s - loss: 3.0515 - acc: 0.083 - ETA: 142s - loss: 3.0710 - acc: 0.081 - ETA: 141s - loss: 3.0692 - acc: 0.085 - ETA: 140s - loss: 3.0512 - acc: 0.099 - ETA: 139s - loss: 3.0435 - acc: 0.096 - ETA: 138s - loss: 3.0419 - acc: 0.098 - ETA: 137s - loss: 3.0493 - acc: 0.100 - ETA: 136s - loss: 3.0570 - acc: 0.093 - ETA: 135s - loss: 3.0719 - acc: 0.091 - ETA: 134s - loss: 3.0763 - acc: 0.093 - ETA: 134s - loss: 3.0783 - acc: 0.098 - ETA: 133s - loss: 3.0805 - acc: 0.109 - ETA: 132s - loss: 3.0767 - acc: 0.113 - ETA: 131s - loss: 3.0820 - acc: 0.110 - ETA: 130s - loss: 3.0805 - acc: 0.111 - ETA: 130s - loss: 3.0800 - acc: 0.109 - ETA: 129s - loss: 3.0843 - acc: 0.112 - ETA: 128s - loss: 3.0762 - acc: 0.122 - ETA: 127s - loss: 3.0808 - acc: 0.120 - ETA: 126s - loss: 3.0804 - acc: 0.120 - ETA: 126s - loss: 3.0792 - acc: 0.118 - ETA: 125s - loss: 3.0734 - acc: 0.122 - ETA: 124s - loss: 3.0660 - acc: 0.125 - ETA: 123s - loss: 3.0698 - acc: 0.121 - ETA: 123s - loss: 3.0703 - acc: 0.119 - ETA: 122s - loss: 3.0739 - acc: 0.119 - ETA: 121s - loss: 3.0771 - acc: 0.119 - ETA: 120s - loss: 3.0769 - acc: 0.118 - ETA: 120s - loss: 3.0774 - acc: 0.118 - ETA: 119s - loss: 3.0809 - acc: 0.120 - ETA: 118s - loss: 3.0844 - acc: 0.120 - ETA: 117s - loss: 3.0800 - acc: 0.121 - ETA: 117s - loss: 3.0781 - acc: 0.120 - ETA: 116s - loss: 3.0767 - acc: 0.119 - ETA: 115s - loss: 3.0729 - acc: 0.120 - ETA: 114s - loss: 3.0710 - acc: 0.120 - ETA: 114s - loss: 3.0729 - acc: 0.122 - ETA: 113s - loss: 3.0718 - acc: 0.120 - ETA: 112s - loss: 3.0735 - acc: 0.118 - ETA: 111s - loss: 3.0727 - acc: 0.123 - ETA: 111s - loss: 3.0728 - acc: 0.123 - ETA: 110s - loss: 3.0691 - acc: 0.125 - ETA: 109s - loss: 3.0712 - acc: 0.122 - ETA: 108s - loss: 3.0693 - acc: 0.121 - ETA: 108s - loss: 3.0682 - acc: 0.123 - ETA: 107s - loss: 3.0710 - acc: 0.122 - ETA: 106s - loss: 3.0720 - acc: 0.121 - ETA: 105s - loss: 3.0739 - acc: 0.120 - ETA: 105s - loss: 3.0748 - acc: 0.118 - ETA: 104s - loss: 3.0740 - acc: 0.117 - ETA: 103s - loss: 3.0741 - acc: 0.117 - ETA: 103s - loss: 3.0725 - acc: 0.117 - ETA: 102s - loss: 3.0709 - acc: 0.117 - ETA: 101s - loss: 3.0703 - acc: 0.117 - ETA: 100s - loss: 3.0712 - acc: 0.117 - ETA: 100s - loss: 3.0730 - acc: 0.118 - ETA: 99s - loss: 3.0723 - acc: 0.119 - ETA: 98s - loss: 3.0733 - acc: 0.11 - ETA: 97s - loss: 3.0724 - acc: 0.12 - ETA: 97s - loss: 3.0743 - acc: 0.12 - ETA: 96s - loss: 3.0782 - acc: 0.12 - ETA: 95s - loss: 3.0804 - acc: 0.12 - ETA: 94s - loss: 3.0786 - acc: 0.12 - ETA: 94s - loss: 3.0801 - acc: 0.12 - ETA: 93s - loss: 3.0818 - acc: 0.12 - ETA: 92s - loss: 3.0802 - acc: 0.12 - ETA: 91s - loss: 3.0810 - acc: 0.12 - ETA: 91s - loss: 3.0794 - acc: 0.12 - ETA: 90s - loss: 3.0790 - acc: 0.12 - ETA: 89s - loss: 3.0821 - acc: 0.12 - ETA: 88s - loss: 3.0809 - acc: 0.12 - ETA: 88s - loss: 3.0808 - acc: 0.12 - ETA: 87s - loss: 3.0825 - acc: 0.11 - ETA: 86s - loss: 3.0807 - acc: 0.12 - ETA: 85s - loss: 3.0806 - acc: 0.12 - ETA: 85s - loss: 3.0815 - acc: 0.11 - ETA: 84s - loss: 3.0829 - acc: 0.11 - ETA: 83s - loss: 3.0834 - acc: 0.11 - ETA: 83s - loss: 3.0847 - acc: 0.11 - ETA: 82s - loss: 3.0821 - acc: 0.12 - ETA: 81s - loss: 3.0799 - acc: 0.12 - ETA: 80s - loss: 3.0796 - acc: 0.12 - ETA: 80s - loss: 3.0798 - acc: 0.12 - ETA: 79s - loss: 3.0801 - acc: 0.12 - ETA: 78s - loss: 3.0791 - acc: 0.12 - ETA: 77s - loss: 3.0799 - acc: 0.12 - ETA: 77s - loss: 3.0803 - acc: 0.12 - ETA: 76s - loss: 3.0806 - acc: 0.12 - ETA: 75s - loss: 3.0785 - acc: 0.12 - ETA: 74s - loss: 3.0796 - acc: 0.12 - ETA: 74s - loss: 3.0765 - acc: 0.12 - ETA: 73s - loss: 3.0760 - acc: 0.12 - ETA: 72s - loss: 3.0788 - acc: 0.12 - ETA: 72s - loss: 3.0774 - acc: 0.12 - ETA: 71s - loss: 3.0780 - acc: 0.12 - ETA: 70s - loss: 3.0765 - acc: 0.12 - ETA: 69s - loss: 3.0776 - acc: 0.12 - ETA: 69s - loss: 3.0785 - acc: 0.12 - ETA: 68s - loss: 3.0761 - acc: 0.12 - ETA: 67s - loss: 3.0772 - acc: 0.12 - ETA: 66s - loss: 3.0786 - acc: 0.12 - ETA: 66s - loss: 3.0775 - acc: 0.12 - ETA: 65s - loss: 3.0760 - acc: 0.12 - ETA: 64s - loss: 3.0770 - acc: 0.12 - ETA: 63s - loss: 3.0780 - acc: 0.12 - ETA: 63s - loss: 3.0780 - acc: 0.12 - ETA: 62s - loss: 3.0791 - acc: 0.12 - ETA: 61s - loss: 3.0758 - acc: 0.12 - ETA: 60s - loss: 3.0750 - acc: 0.12 - ETA: 60s - loss: 3.0756 - acc: 0.12 - ETA: 59s - loss: 3.0771 - acc: 0.12 - ETA: 58s - loss: 3.0777 - acc: 0.12 - ETA: 58s - loss: 3.0787 - acc: 0.12 - ETA: 57s - loss: 3.0790 - acc: 0.11 - ETA: 56s - loss: 3.0801 - acc: 0.11 - ETA: 55s - loss: 3.0812 - acc: 0.12 - ETA: 55s - loss: 3.0808 - acc: 0.12 - ETA: 54s - loss: 3.0810 - acc: 0.12 - ETA: 53s - loss: 3.0802 - acc: 0.12 - ETA: 52s - loss: 3.0800 - acc: 0.12 - ETA: 52s - loss: 3.0799 - acc: 0.12 - ETA: 51s - loss: 3.0794 - acc: 0.12 - ETA: 50s - loss: 3.0780 - acc: 0.12 - ETA: 49s - loss: 3.0768 - acc: 0.12 - ETA: 49s - loss: 3.0773 - acc: 0.12 - ETA: 48s - loss: 3.0777 - acc: 0.12 - ETA: 47s - loss: 3.0785 - acc: 0.12 - ETA: 47s - loss: 3.0785 - acc: 0.12 - ETA: 46s - loss: 3.0801 - acc: 0.12 - ETA: 45s - loss: 3.0810 - acc: 0.12 - ETA: 44s - loss: 3.0803 - acc: 0.12 - ETA: 44s - loss: 3.0800 - acc: 0.12 - ETA: 43s - loss: 3.0792 - acc: 0.12 - ETA: 42s - loss: 3.0788 - acc: 0.12 - ETA: 41s - loss: 3.0787 - acc: 0.12 - ETA: 41s - loss: 3.0797 - acc: 0.12 - ETA: 40s - loss: 3.0782 - acc: 0.12 - ETA: 39s - loss: 3.0764 - acc: 0.12 - ETA: 38s - loss: 3.0760 - acc: 0.12 - ETA: 38s - loss: 3.0763 - acc: 0.12 - ETA: 37s - loss: 3.0776 - acc: 0.12 - ETA: 36s - loss: 3.0771 - acc: 0.12 - ETA: 35s - loss: 3.0765 - acc: 0.12 - ETA: 35s - loss: 3.0769 - acc: 0.12 - ETA: 34s - loss: 3.0765 - acc: 0.11 - ETA: 33s - loss: 3.0749 - acc: 0.12 - ETA: 33s - loss: 3.0755 - acc: 0.12 - ETA: 32s - loss: 3.0740 - acc: 0.12 - ETA: 31s - loss: 3.0734 - acc: 0.12 - ETA: 30s - loss: 3.0748 - acc: 0.12 - ETA: 30s - loss: 3.0754 - acc: 0.12 - ETA: 29s - loss: 3.0763 - acc: 0.11 - ETA: 28s - loss: 3.0760 - acc: 0.11 - ETA: 27s - loss: 3.0761 - acc: 0.12 - ETA: 27s - loss: 3.0752 - acc: 0.12 - ETA: 26s - loss: 3.0766 - acc: 0.11 - ETA: 25s - loss: 3.0763 - acc: 0.12 - ETA: 24s - loss: 3.0769 - acc: 0.12 - ETA: 24s - loss: 3.0779 - acc: 0.11 - ETA: 23s - loss: 3.0774 - acc: 0.11 - ETA: 22s - loss: 3.0765 - acc: 0.12 - ETA: 22s - loss: 3.0774 - acc: 0.11 - ETA: 21s - loss: 3.0763 - acc: 0.12 - ETA: 20s - loss: 3.0754 - acc: 0.12 - ETA: 19s - loss: 3.0744 - acc: 0.12 - ETA: 19s - loss: 3.0747 - acc: 0.12 - ETA: 18s - loss: 3.0738 - acc: 0.12 - ETA: 17s - loss: 3.0726 - acc: 0.12 - ETA: 16s - loss: 3.0712 - acc: 0.12 - ETA: 16s - loss: 3.0724 - acc: 0.12 - ETA: 15s - loss: 3.0715 - acc: 0.12 - ETA: 14s - loss: 3.0721 - acc: 0.12 - ETA: 13s - loss: 3.0723 - acc: 0.12 - ETA: 13s - loss: 3.0727 - acc: 0.12 - ETA: 12s - loss: 3.0741 - acc: 0.12 - ETA: 11s - loss: 3.0747 - acc: 0.12 - ETA: 11s - loss: 3.0738 - acc: 0.12 - ETA: 10s - loss: 3.0744 - acc: 0.12 - ETA: 9s - loss: 3.0745 - acc: 0.1227 - ETA: 8s - loss: 3.0735 - acc: 0.122 - ETA: 8s - loss: 3.0729 - acc: 0.122 - ETA: 7s - loss: 3.0720 - acc: 0.122 - ETA: 6s - loss: 3.0718 - acc: 0.122 - ETA: 5s - loss: 3.0722 - acc: 0.121 - ETA: 5s - loss: 3.0720 - acc: 0.121 - ETA: 4s - loss: 3.0721 - acc: 0.121 - ETA: 3s - loss: 3.0709 - acc: 0.122 - ETA: 2s - loss: 3.0715 - acc: 0.122 - ETA: 2s - loss: 3.0713 - acc: 0.122 - ETA: 1s - loss: 3.0711 - acc: 0.121 - ETA: 0s - loss: 3.0704 - acc: 0.1219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skumarravindran\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35gpu1\\lib\\site-packages\\keras\\callbacks.py:405: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 146s - loss: 3.0701 - acc: 0.1216   \n",
      "Epoch 2/2\n",
      "200/200 [==============================] - ETA: 136s - loss: 3.2865 - acc: 0.0000e+0 - ETA: 140s - loss: 3.1555 - acc: 0.0938    - ETA: 141s - loss: 3.0952 - acc: 0.104 - ETA: 141s - loss: 3.0996 - acc: 0.125 - ETA: 140s - loss: 3.0700 - acc: 0.150 - ETA: 140s - loss: 3.0584 - acc: 0.156 - ETA: 140s - loss: 3.0639 - acc: 0.142 - ETA: 139s - loss: 3.0352 - acc: 0.140 - ETA: 139s - loss: 3.0287 - acc: 0.131 - ETA: 138s - loss: 3.0285 - acc: 0.150 - ETA: 137s - loss: 3.0208 - acc: 0.147 - ETA: 137s - loss: 3.0209 - acc: 0.145 - ETA: 136s - loss: 3.0226 - acc: 0.139 - ETA: 135s - loss: 3.0210 - acc: 0.142 - ETA: 135s - loss: 3.0215 - acc: 0.137 - ETA: 134s - loss: 3.0395 - acc: 0.136 - ETA: 133s - loss: 3.0560 - acc: 0.128 - ETA: 132s - loss: 3.0388 - acc: 0.131 - ETA: 132s - loss: 3.0356 - acc: 0.134 - ETA: 131s - loss: 3.0462 - acc: 0.128 - ETA: 130s - loss: 3.0575 - acc: 0.122 - ETA: 130s - loss: 3.0596 - acc: 0.119 - ETA: 129s - loss: 3.0582 - acc: 0.122 - ETA: 128s - loss: 3.0524 - acc: 0.119 - ETA: 127s - loss: 3.0391 - acc: 0.122 - ETA: 127s - loss: 3.0426 - acc: 0.122 - ETA: 126s - loss: 3.0360 - acc: 0.127 - ETA: 125s - loss: 3.0385 - acc: 0.129 - ETA: 125s - loss: 3.0427 - acc: 0.129 - ETA: 124s - loss: 3.0410 - acc: 0.133 - ETA: 123s - loss: 3.0424 - acc: 0.131 - ETA: 122s - loss: 3.0370 - acc: 0.132 - ETA: 122s - loss: 3.0292 - acc: 0.140 - ETA: 121s - loss: 3.0259 - acc: 0.139 - ETA: 120s - loss: 3.0246 - acc: 0.141 - ETA: 120s - loss: 3.0239 - acc: 0.140 - ETA: 119s - loss: 3.0151 - acc: 0.143 - ETA: 118s - loss: 3.0087 - acc: 0.148 - ETA: 117s - loss: 3.0080 - acc: 0.152 - ETA: 117s - loss: 3.0168 - acc: 0.150 - ETA: 116s - loss: 3.0164 - acc: 0.147 - ETA: 115s - loss: 3.0150 - acc: 0.145 - ETA: 114s - loss: 3.0103 - acc: 0.145 - ETA: 114s - loss: 3.0131 - acc: 0.143 - ETA: 113s - loss: 3.0155 - acc: 0.141 - ETA: 112s - loss: 3.0104 - acc: 0.142 - ETA: 112s - loss: 3.0073 - acc: 0.142 - ETA: 111s - loss: 3.0061 - acc: 0.141 - ETA: 110s - loss: 3.0077 - acc: 0.140 - ETA: 109s - loss: 3.0156 - acc: 0.137 - ETA: 109s - loss: 3.0229 - acc: 0.134 - ETA: 108s - loss: 3.0201 - acc: 0.134 - ETA: 107s - loss: 3.0216 - acc: 0.134 - ETA: 106s - loss: 3.0228 - acc: 0.134 - ETA: 106s - loss: 3.0147 - acc: 0.138 - ETA: 105s - loss: 3.0161 - acc: 0.137 - ETA: 104s - loss: 3.0175 - acc: 0.136 - ETA: 104s - loss: 3.0169 - acc: 0.134 - ETA: 103s - loss: 3.0217 - acc: 0.133 - ETA: 102s - loss: 3.0242 - acc: 0.132 - ETA: 101s - loss: 3.0183 - acc: 0.135 - ETA: 101s - loss: 3.0147 - acc: 0.136 - ETA: 100s - loss: 3.0161 - acc: 0.134 - ETA: 99s - loss: 3.0162 - acc: 0.136 - ETA: 98s - loss: 3.0162 - acc: 0.13 - ETA: 98s - loss: 3.0165 - acc: 0.13 - ETA: 97s - loss: 3.0130 - acc: 0.13 - ETA: 96s - loss: 3.0132 - acc: 0.13 - ETA: 95s - loss: 3.0119 - acc: 0.14 - ETA: 95s - loss: 3.0153 - acc: 0.13 - ETA: 94s - loss: 3.0192 - acc: 0.13 - ETA: 93s - loss: 3.0210 - acc: 0.13 - ETA: 93s - loss: 3.0178 - acc: 0.13 - ETA: 92s - loss: 3.0153 - acc: 0.13 - ETA: 91s - loss: 3.0167 - acc: 0.13 - ETA: 90s - loss: 3.0203 - acc: 0.13 - ETA: 90s - loss: 3.0194 - acc: 0.13 - ETA: 89s - loss: 3.0179 - acc: 0.13 - ETA: 88s - loss: 3.0166 - acc: 0.13 - ETA: 87s - loss: 3.0137 - acc: 0.14 - ETA: 87s - loss: 3.0153 - acc: 0.14 - ETA: 86s - loss: 3.0167 - acc: 0.14 - ETA: 85s - loss: 3.0161 - acc: 0.14 - ETA: 85s - loss: 3.0153 - acc: 0.14 - ETA: 84s - loss: 3.0163 - acc: 0.14 - ETA: 83s - loss: 3.0160 - acc: 0.14 - ETA: 82s - loss: 3.0155 - acc: 0.14 - ETA: 82s - loss: 3.0153 - acc: 0.14 - ETA: 81s - loss: 3.0147 - acc: 0.14 - ETA: 80s - loss: 3.0165 - acc: 0.14 - ETA: 79s - loss: 3.0166 - acc: 0.14 - ETA: 79s - loss: 3.0162 - acc: 0.14 - ETA: 78s - loss: 3.0162 - acc: 0.14 - ETA: 77s - loss: 3.0176 - acc: 0.14 - ETA: 76s - loss: 3.0184 - acc: 0.14 - ETA: 76s - loss: 3.0196 - acc: 0.14 - ETA: 75s - loss: 3.0198 - acc: 0.14 - ETA: 74s - loss: 3.0182 - acc: 0.14 - ETA: 74s - loss: 3.0180 - acc: 0.14 - ETA: 73s - loss: 3.0171 - acc: 0.14 - ETA: 72s - loss: 3.0181 - acc: 0.14 - ETA: 71s - loss: 3.0193 - acc: 0.14 - ETA: 71s - loss: 3.0206 - acc: 0.14 - ETA: 70s - loss: 3.0215 - acc: 0.14 - ETA: 69s - loss: 3.0186 - acc: 0.14 - ETA: 68s - loss: 3.0178 - acc: 0.14 - ETA: 68s - loss: 3.0169 - acc: 0.14 - ETA: 67s - loss: 3.0183 - acc: 0.14 - ETA: 66s - loss: 3.0163 - acc: 0.14 - ETA: 65s - loss: 3.0166 - acc: 0.14 - ETA: 65s - loss: 3.0196 - acc: 0.14 - ETA: 64s - loss: 3.0186 - acc: 0.14 - ETA: 63s - loss: 3.0193 - acc: 0.14 - ETA: 63s - loss: 3.0198 - acc: 0.13 - ETA: 62s - loss: 3.0188 - acc: 0.13 - ETA: 61s - loss: 3.0167 - acc: 0.14 - ETA: 60s - loss: 3.0136 - acc: 0.14 - ETA: 60s - loss: 3.0131 - acc: 0.14 - ETA: 59s - loss: 3.0129 - acc: 0.14 - ETA: 58s - loss: 3.0123 - acc: 0.14 - ETA: 57s - loss: 3.0122 - acc: 0.14 - ETA: 57s - loss: 3.0115 - acc: 0.13 - ETA: 56s - loss: 3.0097 - acc: 0.14 - ETA: 55s - loss: 3.0114 - acc: 0.13 - ETA: 54s - loss: 3.0093 - acc: 0.14 - ETA: 54s - loss: 3.0065 - acc: 0.14 - ETA: 53s - loss: 3.0083 - acc: 0.13 - ETA: 52s - loss: 3.0090 - acc: 0.13 - ETA: 52s - loss: 3.0080 - acc: 0.13 - ETA: 51s - loss: 3.0089 - acc: 0.13 - ETA: 50s - loss: 3.0098 - acc: 0.13 - ETA: 49s - loss: 3.0110 - acc: 0.13 - ETA: 49s - loss: 3.0086 - acc: 0.13 - ETA: 48s - loss: 3.0083 - acc: 0.13 - ETA: 47s - loss: 3.0093 - acc: 0.13 - ETA: 46s - loss: 3.0113 - acc: 0.13 - ETA: 46s - loss: 3.0112 - acc: 0.13 - ETA: 45s - loss: 3.0115 - acc: 0.13 - ETA: 44s - loss: 3.0117 - acc: 0.13 - ETA: 43s - loss: 3.0129 - acc: 0.13 - ETA: 43s - loss: 3.0133 - acc: 0.13 - ETA: 42s - loss: 3.0143 - acc: 0.13 - ETA: 41s - loss: 3.0117 - acc: 0.13 - ETA: 41s - loss: 3.0134 - acc: 0.13 - ETA: 40s - loss: 3.0146 - acc: 0.13 - ETA: 39s - loss: 3.0153 - acc: 0.13 - ETA: 38s - loss: 3.0162 - acc: 0.13 - ETA: 38s - loss: 3.0175 - acc: 0.13 - ETA: 37s - loss: 3.0173 - acc: 0.13 - ETA: 36s - loss: 3.0162 - acc: 0.13 - ETA: 35s - loss: 3.0160 - acc: 0.13 - ETA: 35s - loss: 3.0166 - acc: 0.13 - ETA: 34s - loss: 3.0164 - acc: 0.13 - ETA: 33s - loss: 3.0180 - acc: 0.13 - ETA: 32s - loss: 3.0179 - acc: 0.13 - ETA: 32s - loss: 3.0196 - acc: 0.13 - ETA: 31s - loss: 3.0203 - acc: 0.13 - ETA: 30s - loss: 3.0199 - acc: 0.13 - ETA: 30s - loss: 3.0215 - acc: 0.13 - ETA: 29s - loss: 3.0220 - acc: 0.13 - ETA: 28s - loss: 3.0220 - acc: 0.13 - ETA: 27s - loss: 3.0235 - acc: 0.13 - ETA: 27s - loss: 3.0231 - acc: 0.13 - ETA: 26s - loss: 3.0229 - acc: 0.13 - ETA: 25s - loss: 3.0228 - acc: 0.13 - ETA: 24s - loss: 3.0215 - acc: 0.13 - ETA: 24s - loss: 3.0229 - acc: 0.13 - ETA: 23s - loss: 3.0221 - acc: 0.13 - ETA: 22s - loss: 3.0218 - acc: 0.13 - ETA: 21s - loss: 3.0216 - acc: 0.13 - ETA: 21s - loss: 3.0213 - acc: 0.13 - ETA: 20s - loss: 3.0204 - acc: 0.13 - ETA: 19s - loss: 3.0198 - acc: 0.14 - ETA: 19s - loss: 3.0197 - acc: 0.13 - ETA: 18s - loss: 3.0173 - acc: 0.14 - ETA: 17s - loss: 3.0167 - acc: 0.14 - ETA: 16s - loss: 3.0171 - acc: 0.14 - ETA: 16s - loss: 3.0186 - acc: 0.14 - ETA: 15s - loss: 3.0197 - acc: 0.13 - ETA: 14s - loss: 3.0200 - acc: 0.13 - ETA: 13s - loss: 3.0201 - acc: 0.14 - ETA: 13s - loss: 3.0205 - acc: 0.13 - ETA: 12s - loss: 3.0203 - acc: 0.13 - ETA: 11s - loss: 3.0216 - acc: 0.13 - ETA: 10s - loss: 3.0194 - acc: 0.13 - ETA: 10s - loss: 3.0189 - acc: 0.13 - ETA: 9s - loss: 3.0174 - acc: 0.1407 - ETA: 8s - loss: 3.0171 - acc: 0.140 - ETA: 8s - loss: 3.0178 - acc: 0.140 - ETA: 7s - loss: 3.0171 - acc: 0.140 - ETA: 6s - loss: 3.0175 - acc: 0.139 - ETA: 5s - loss: 3.0159 - acc: 0.141 - ETA: 5s - loss: 3.0172 - acc: 0.140 - ETA: 4s - loss: 3.0161 - acc: 0.139 - ETA: 3s - loss: 3.0161 - acc: 0.139 - ETA: 2s - loss: 3.0159 - acc: 0.140 - ETA: 2s - loss: 3.0163 - acc: 0.139 - ETA: 1s - loss: 3.0159 - acc: 0.139 - ETA: 0s - loss: 3.0145 - acc: 0.140 - 146s - loss: 3.0171 - acc: 0.1403   \n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc', save_best_only=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## predict test data\n",
    "predictions = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-09dd70b7a1c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrev_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrev_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\skumarravindran\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35gpu1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     \"\"\"\n\u001b[1;32m--> 963\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\skumarravindran\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35gpu1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# get labels\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "rev_y = {v:k for k,v in Y_train.items()}\n",
    "pred_labels = [rev_y[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make submission\n",
    "sub = pd.DataFrame({'image_id':test.image_id, 'label':pred_labels})\n",
    "sub.to_csv('sub_vgg1.csv', index=False) ## ~0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
